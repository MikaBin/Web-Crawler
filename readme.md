# Web Crawler (웹 크롤러)

> 목표: **안전하고 확장 가능한 크롤링 파이프라인**을 만들기 위한 개인용/팀용 웹 크롤러 프로젝트  
> ⚠️ 이 프로젝트는 **합법적이고 허용된 범위**에서 진행됨.

---

## 무엇을 하는 프로젝트야?

이 크롤러는 지정한 URL(또는 사이트맵/목록)을 기반으로 웹 페이지를 순회하면서 데이터를 수집하고, 정제한 뒤 **일관된 포맷(JSON/CSV/DB 등)** 으로 저장하는 걸 목표로 해.

- “일단 긁어오자”가 아니라,
- **속도/안정성/재시도/중복방지/확장성/로그**까지 기본으로 깔아두는 방향.

---

## 주요 기능 (예정/구현)

- [ ] URL 큐 기반 크롤링 (BFS/DFS, depth 제한)
- [ ] 중복 방지 (URL 정규화 + 해시)
- [ ] Rate Limit / Throttling (도메인별 속도 제한)
- [ ] 재시도/백오프/타임아웃
- [ ] robots.txt / 사이트 정책 존중 옵션
- [ ] User-Agent, 헤더, 프록시 설정
- [ ] HTML 파싱 + 데이터 추출(Selector/XPath/Regex)
- [ ] 결과 저장: JSON/CSV/DB(선택)
- [ ] 실행 로그/에러 리포트/수집 통계

---

## 빠른 시작 (Quick Start)
